\section{Introduction}
In scientific research \cite{hisano2013challenges}, software engineering and cybersecurity \cite{littlewood1989predicting,maillart2017given}, politics \cite{clinton2014hard}, and daily life \cite{gerson1986hard}, individuals face problems that involve many interdependent variables and large problem spaces \cite{koller09, Pearl2009CMR}.These problems are known to be hard to humans and have been studied as such by cognitive scientists. Baysian Networks are used as models for human causal inference and resoning \cite{bramley2015staying, castnerForthcoming, Griffiths2008, Pearl88, Spiegler2016, spiegler2015}. Not only are these problems hard to humans, but they are also hard to computers, as most inferences in Probabilistic Graphical Models (of which Bayes Nets are a part) are NP hard \cite{koller09}. This difficulty explains diversity of models and is often at least partially resolvable through model ensembles and forms of model averaging.\\ 

Modern theories of cognition often start with the premise that the brain is in large part an inference engine \cite{Tenenbaum06theory-basedbayesian} and causal cognition is theorized as the construction of complex causal theories from extremely sparse observations. In this literature, simple experiments were designed to probe cognitive capabilities \cite{tenenbaum2001structure}. The conclusion that emerges is that the brain cannot simply be an information processor; the information is too sparse to spell out complete theories and the theories that humans construct are often better than what the observations alone can account for \cite{ortoleva2012modeling, Hong04} \\

However, there is still a dearth of knowledge about the dynamics of human learning, or even of Bayesian learning in high dimensional spaces where near zero prior weights are given to joint events that are in truth more likely than initially theorized \cite{ortoleva2012modeling}. People tend to initially underweigh unlikely events \cite{taleb2007} and this initial underweighing can have very longterm consequences for beliefs, even if updating is perfectly Bayesian.\\

In more detail, what is poorly understood is how cognitive exploration brings new understanding, which is then recombined with previously acquired knowledge, stored in memory (see Figure \ref{fig:2}A). Exploration occurs by leveraging memory (see Figure \ref{fig:2}B). While exploration entails a component of chance (beyond the previous cognitive frontier), recombination can be optimised to find the best possible recombination, which corresponds to the optimal proposed solution (i.e., as close as possible, in distance, to the true solution) within the cognitive frontier.\\

Here, we investigate the fine-grained cognitive mechanisms of complex problem resolution, which involves 2 treatments {\bf for} the resolution of 3-node and 4-node Bayesian networks over one trial of 30 minutes, with a warm-up period of 10 minutes. Reverse-engineering these Bayesian networks involves evaluating respectively 8 and 16 joint probabilities (each between $0$ and $1$, normalized such that they all add up to $1$). Any change to any of these probabilities is registered with a resolution of one second [see Supplementary Information (SI) \ref{SI_experiment}].\\

Participants are incentivized via a version of the Becker–DeGroot–Marschak method--a incentive-compatible procedure commonly used in experimental economics--to measure the willingness to pay (WTP) for some good. Here, the price is a self-decided probability and the good is a sort of security.  This security is worth \$ $1$, if the underlying "asset" (a special variable with a label, the identity of which switches at random at 1 minute intervals) attains the value ``H'' (for ``high'') and \$ $0$, if it attains the value ``L'' for ``low''. The more of this security is bought, the higher the price. It is incentive competible to buy exactly as much of this good such that the probability according to one's belief, matches the price. In the game, participants simply set their probabilities and the mechanism matches the shares automatically to the probabilities.  This special variable--which changes from minute to minute--is known to be entangled, causally with all the other variables, but the causal structure is not given.  In expecatitions--and this is known to and understood by all players--it pays to understand and model correctly, the causal structure of the respectively 3 or 4 variable system of covarying variables.  The best evidence that we have that people really understood this principle is that most models became better over time.  \\

The experiment we performed here is more complex than is typical in this literature in the following ways : participants are given monetary incentives for the truthful revelation and cognitively demanding refinements of their beliefs in a way that is common only in experimental economics.  In that way, the experiment is a bridge between experimental cognitive science and a new economics of cognition that is influenced by both, computer science and cognitive science \cite{Spiegler2016}. Important to this new literature is the explicit theoretical recognition of the seemingly obvious fact that information is not necessarily interpreted in the same way, even if all individuals are privy to the same exact information and only to that common information. The experiment from which our data stems, then, belongs to a literature that is situated in the convergence between Herbert Simon's classic theory of ``bounded rationality'' in economics \cite{Gigerenzer2001, Rubinstein98, tsang2008computational, simon1955behavioral} and modern theories of intelligence that are derived from attempts to engineer intelligence.\\  

In realistic stock market scenarios, for example, most information is public information, available to all, and thus the markets cannot possibly be mere information aggregation mechanisms. The complexity and the uncertainties of the information environment in such systems leave a lot to interpretation and it seems clear that these differences in interpretation account for much more opinion heterogeneities in most human affairs than the differences in information exposure alone. Thus, markets are really perspective aggregators, rather than mere information aggregators \cite{hong2009interpreted}. Beliefs can be out of sync with reality over long periods of time, yet often if beliefs are diverse, when they are aggregated they can collectively get suprisingly closer to the truth. \\


In our experiment, on average, participants performed poorly in both treatments (see Figure \ref{fig:1}A), and improvement of the proposed models over time follows an extremely slow decay [with Jensen-Shannon Distance $D_{jsd}(t) \sim t^{\nu}$ with $ \nu \approx -0.15(1)$]. \\

{\bf [to be extended and refined as results converge:]} The experiment reveals fine grained mechanisms of how people struggle to balance recombination of mental structures stored in memory with exploration beyond their current cognitive frontier.  Our results suggest that displacement $0.1 < \Delta r < 0.2 $ is particularly beneficial for making progress toward the correct solution. We also find that large displacements seem to require orders of magnitude more ``cognitive processing'' time compared to small displacements. Displacement $0.1 < \Delta r < 0.2 $ is precisely at the inflexion point before waiting times get punishingly long.\\

{\bf [remants to be integrated elsewhere : ]} We first report on the experimental results, in particular deviations from a memoryless L\'evy Walks/Flights, such as peculiar returns to previously visited solutions, {\bf anomalous mean square displacement following return and recombination [more work is needed here]}, explorations beyond the cognitive frontier, as well as waiting-time and long-memory processes. What emerges from our observations is an empirically driven psychological theory of the cognitive costs of new ideas. These costs prevent optimality, as there is nothing special about the ideas that have already occurred, with respect to reality and the possibilities as to how things might actually work. It is important to note that there is a macro-parallel here with Thomas S. Kuhn's ``The Structure of Scientific Revolutions''--also, in a less direct and perhaps deeper level, to Stephen Jay Gould's theory of evolution by punctuated equilibria. Less direct only because Bayesian Nets are really quite like the ``scientific paradigms'' in Kuhn but they are only indirectly or metaphorically like evolutionary ``rugged fitness landscapes''. We then show how memory, exploration and recombination influence performance. {\bf Building on theoretical consideration in conjunction with observed stylized facts, we test a model of mechanics of cognition in situations in which people tackle hard problems [remains to be done. It may incorporate some Hawkes Processes, but not 100\% sure yet]}. 

This article is organized as follows. As for background, we first review (optimal) search processes in nature and society, and how they connect to causal inference {\bf [not sure if we can do that actually, because they processes are typically memoryless, and do not necessarily involve memory, exception made of [c.f. article in ipad]]}. We then present the specifics of the conducted experiment, and its results, including indications of (i) {\it an anomalous super-diffusive process},  (ii) the {\it exploit vs. explore} phenomenon, and (iii) {\it memory, return to previously visited sites \& recombination}, followed by the discussion. 





