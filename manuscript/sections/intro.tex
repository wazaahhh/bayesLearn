\section{Introduction}
When searching for sparse (resp. unique) solutions in complex problem spaces, animals and humans seem to use search strategies that are best modeled as L\'evy walks or flights.  These are random walk processes; where the distance and direction of a move in space are random variables following particular distributions.  Each move can be short range (local) or long-range.  Optimized random search is a class of models built to explain natural search processes such as animal or human movement through space.  The argument is that humans or animals of all sorts have evolved behavioral patterns that are optimal for the maximal finding of scarce resources.  In some natural spaces, optimal strategies are L\'evy flights; alternating clusters of many local displacements with punctual long range displacements \cite{viswanathan1999optimizing, edwards2007revisiting,song2010modelling,viswanathan2011physics}.  More precisely, optimal strategies in various environments suggest different power-law step-size distributions $P(l) ~ l^{-\mu}$, characterized by a range of power-law exponents $\mu_{opt}$.  If $\mu_{opt}\rightarrow 1$, the process is known as ballistic motion, if $1<\mu_{opt}\leq3$ the process is a L\'evy flight and if $\mu_{opt}\geq 3$ then it is known as Brownian motion. 

L\'evy flights are found to empirically describe search strategies for food in large areas for numerous species, and it has been shown that hunter-gatherers as well as modern urbanites behave similarly when they move through space.  The movement patterns of urbanites are traced from their banknotes \cite{brockmann2006scaling} and mobile phones \cite{gonzalez2008understanding,song2010modelling}. 
\\

The cognitive as well as evolutionary \cite{radicchi2012evolution} underpinnings of human mobility patterns in physical spaces, that are best described by L\'evy walks, have been questioned and investigated.  Here we question if humans resort to similar strategies when they search for solutions to problems in more abstract spaces.  Recent research on online bids \cite{radicchi2012rationality} found similar L\'evy walk patterns even though they appear sup-optimal and irrational for common models of optimality and rationality.  These findings suggest that L\'evy walks might be hard coded in human cognition \cite{radicchi2012evolution}, as humans use search strategies which do not appear fitness maximizing in modern abstract information and reasoning environments in which solving problems brings rewards in the form of money, recognition, reputation and pleasure \cite{rewards_modern_societies}.\\

Considering humans facing hard problems -- cases involving specific types of auctions --  \cite{baronchelli2013levy} has found that cognitive mental searches give rise to observable patterns \cite{rhodes2007human,radicchi2012rationality,radicchi2012evolution} that resemble the patterns resulting from ancient foraging and mobility processes. These search processes are best modeled by L\'evy walks/flights and are optimal in physical spaces with sparse food distributions.  Thus, in this literature it is argued that the cognitive processes governing human cognitive search through abstract solution spaces likely are holdovers from an earlier evolutionary time and that the brain regions that implement the behavior are likely to be evolutionarily old (Baronchelli \& Radicchi, 2013).  \\
However, only true L\'evy walks/flights are optimal in these environments. True L\’evy processes are memoryless and thus such processes allow exploring the solution space with no consideration of prior knowledge (an evolutionary framework is used to rationalize their findings).\\

Assuming that human cognitive processes are memory-less is problematic however, because evidently human cognitive processes exhibit both short and long term memory.  In fact, it has been shown that memory retrieval itself also follows L\’evy flight behavior (Rhodes and Turvey, 2007), where $P(\tau_j) \dist \tau_j^{-\mu}$, where $\tau_j$ is the $j$s interval of time between retrieved objects and $1<\mu \leq 3$.  Memory, when building cognitive models of stochastic systems, allows humans to combine past experience with explorations of new potential solutions. \footnote{One instance of memory is \
return to previous site (resp. previous solutions), evidence from mobility shows -- not surprisingly -- that people tend to return to previously visited sites \cite{}.}\\

{\bf [a truly cognitive science paragraph is needed here]}

Empirically, approaching a unique solution involves trial and error and progressive learning. One may model an evolutionary process, then, as combinations of Markov processes \cite{} or other long range memory processes in which past candidate solutions  are reused and recombined and L\'evy flight processes to find novel solutions by exploring wildly outside of the envelop of previously explored territory.\\

We conducted an experiment in which participants reverse-engineered the influence structure of two kinds of simple 3 and 4 variable joint stochastic processes, by expressing them explicitly as Bayesian networks. We find that in this setting, the participants’ search strategies alternated between local search patterns and long range relocations (i.e., {\it flights}) over the solution space. Yet they tended to return preferentially to already visited locations.\\

The human tendency to return to previously visited locations, {\it stickiness,}\
 limits the capacity to explore the broader solution space in order to get closer to the solution. As such this behavior may be considered to stem from a genuine cognitive limitation constraining the speed of convergence to the correct solution.  Indeed the pattern that we distill from our data exhibits a particularly slow decay (see Figure \ref{fig:vs_dr}A).  While in abstract spaces returning to preciously exploited locations is most often suboptimal and inefficient, from an evolutionary perspective returning to previous locations in geographic space makes sense.  A good hunting spot is a place with food renewal which creates incentives to regularly return to check for newly available prey: a lion returns regularly to a river bank where zebras get fresh water frequently. \\

 {\bf [slow convergence we find that the decay of the distance to the true model follows a power law $\sim t^{-\nu}$ with $\nu \approx 0.15(1)$]}

\subsection{Experiment}
In our experiment, participants were asked to reverse engineer a multidimensional stochastic process, explicitly expressed in the form of a Bayesian network. They were given 40 minutes, and all changes made were recorded at a 1 second resolution. Participants trying to find the best solution faced a though problem: The {\it simple} process had 3 binary variables, which means that the problem can be thought of as the simultaneous estimation of (2^3) 8 parameters, or the 8 dimensional vector $\mathbf{s}$ with $0 \l\eqslant s_k  \leqslant 1$ for $k = \{1,...,8\}$ (resp. $k = \{1,..., 16\}$ for the 4 variable {\it complex} stochastic system).


\subsection{L\'evy Flight / CTRW}
We first find that the search process approximately follows a continuous time L\'evy flight process, where waiting times between moves are random variables.  Both the distributions of displacement (Figure \ref{fig:pdfs}A ) and waiting times (Figure \ref{fig:pdfs}B ) are best described by power law distributions  (Probability density function of displacement $pdf(\Delta r) = \Delta r^{-\alpha -1}$ with $\alpha = 0.40(5)$.  Note that this is NOT a L\’evy flight (for which $\alpha$ would have to be between 1 and 3.  {\bf B.} Probability density function of waiting time $\Delta t$ $pdf(\Delta t) = \Delt\
a t^{-\beta -1}$ with 2 regimes : $\beta_{\Delta t < 125} = 0.38(4)$ and $\beta_{\Delta t > 125} = 1.59(5)$. Distributions of $\Delta r$ and $\Delta t$ are equivalent for the simple and complex treatments.)

{\bf Problem :} Such CTRW process should normally follow ballistic diffusion characterized by mean square displacement (MSD) and diffusion $\sim t^{\mu}$ with  $\mu_{Levy} = 1$ \
or super-diffusion $\mu_{CTRW} = \beta$ \cite{21,23}). Here, however, mean square displacement (MSD) decays as $\sim t^{\mu}$ with $\mu_{simple} =-0.23(2)$ and $\mu_{complex} =-\
 0.26(1)$ showing a much slower convergence than would be expected.





