\section{Introduction}
<<<<<<< HEAD
Unlike hunter gatherers who typically  roam the environment in search for food \cite{}, modern humans make a living, earn power and compete by searching and making sense of information \cite{}. Competitive advantage from better solutions leads for instance to increased profit derived from a wise investment strategy in finance \cite{}, better products launched on the market \cite{}, or to gain a strategic advantage in an major political election \cite{obama_ict}. \\

However, humans suffer notorious bounded rationality \cite{simon} and cognitive biases \cite{}, which have been shown to severely impede capacity to find good solutions, in particular when people are presented with a problem  to reverse engineer, involving a large solution space \cite{}.\\

Here, we have conducted an experiment in which participants were requested to reverse-engineer the influence structure of two kinds of simple Bayesian networks (made of resp. 3 and 4 nodes). We find that humans search strategies alternate local search patterns with long range relocations (i.e., {\it flights}) over the solution space. These patterns are reminiscent of food search strategies in ecology \cite{}, and in particular by hunter-gatherers \cite{}.\\ 

Indeed, we find that ...  $p(\Delta r) \sim |\Delta r | ^{-\alpha -1}$ (log normal may be more accurate see Figure \ref{fig:jump_sizes_new}) and $p(\Delta t) \sim  |\Delta t | ^{-\beta -1}$ with $\alpha \approx 0.61$ and $\beta \approx 1.8$ (c.f. Figures \ref{fig:jump_sizes} and \ref{fig:waiting_times}). The random variables $\Delta r$ and $\Delta t$ are pretty much uncorrelated (Supplementary Section \ref{si:corr_dr_dt}).\\

However, and on the contrary to former theories (say which ones), the search path is not memoryless: humans tend to return to previously visited locations. This {\it stickiness} limits the capacity to explore the broader solution space in order to get closer to the solution. As such this behavior may be considered as a genuine cognitive bias limiting the convergence to the correct solution, which indeed exhibits a particularly slow decay (see Figure \ref{fig:decay})  However, from an evolutionary perspective, sticking to some places makes sense. A good hunting spot is a place where there is food renewal, which creates incentives to regularly return to check out for newly available prey: Typically, a lion may return regularly to a river bank where zebras get fresh water frequently. \\

Searching for solutions to complicated problems is different to hunting in that it requires to constantly explore new solution sites, even if finding new solution sites is not a completely random process. It may rather stem from the recombination of previous explorations as part of an iterative process \cite{}. \\

In our experiment, participants were asked to reverse engineer a Bayesian network and its influence edges with two treatments : {\it simple} (3 nodes) and {\it complex} (4 nodes). They were given 40 minutes, and any change was recorded at a 1 second resolution.\\

Participants trying to reverse the best solution face a though problem: The {\it simple} Bayesian network has 3 nodes, and is defined by a 8-parameters vector $\mathbf{s}$ with $0 \leqslant s_k  \leqslant 1$ for $k = \{1,...,8\}$ (resp. $k = \{1,..., 16\}$ for the 4 node {\it complex} Bayesian network). To determine return to previous solution sites, we consider a grid partition of the solution space in 4 (resp. 16) dimensional space, with $\bar{s_k} = \{0.01,0.02, .., 1\}$ (At this resolution level, the solution space is thus $10^{9}$ sites for the simple Bayesian network (resp. $10^{16}$ sites in the complex treatment). Distance between two sites is measured as $\Delta r_{i,j} = \| \mathbf{s}_j - \mathbf{s}_i \| = \sqrt{\sum_{k=1}^{k=8} (\mathbf{s}_{j,k} - \mathbf{s}_{i,k})^{2}}$ for $k = \{1,..,8\}$ (resp. $k = \{1,..,16\})$. With a $0.01$ grid resolution the maximum $\epsilon_{\Delta r} = 0.01 \times \sqrt{2} \approx 2.8\%$.\\

The probability to find the true solution is a direct function of the probability to explore a significant portion of the solution space, and to get arbitrarily close to the true solution, through random search alternating short- and long-distance jumps. We observe that the number of new locations visited over time follows 

\begin{equation}
S(t)  \sim t^{\mu},
\end{equation}

with $\mu \approx 0.7$. This is at odds with typical random search strategies involving long range {\it flights}: for L\'evy flights $\mu = 1$ \cite{} and for Continuous Time Random Walk $\mu = \beta$ \cite{}). Since $\Delta r$ and $\Delta t$ are un-correlated, the sub-linear scaling visitation of new sites cannot be attributed to increased delays between jumps over time [i.e., $P(\Delta t|t) = P(\Delta t)$]. Similarly, $P(\Delta r|r(t)) = P(\Delta r)$. The only explanation for the convex nature of $S$ is the return to previously visited sites. Figure \ref{} shows the probability distribution of site visitation  $V$, which best described by a power law distribution 

\begin{equation}
P(V > v) \sim v^{-\gamma}
\end{equation}

with $\gamma \approx 2$. In contrast, the probability $V$ of visitation is expected to be asymptotically ($t \rightarrow \infty$) uniform (P(V) ~ const.)\\

{\bf Finally, ultra slow diffusion: }



\clearpage

From financial stability to the stability of democracies, beliefs play a central role in our explanation of many phenomena. In the social sciences, these beliefs are often conceptualized as probabilistic assessments over states of the world.  However, they are derived from coherent belief systems people hold in their minds regarding how the world works as supported by recent work in cognitive science \cite{lombrozo2006structure, anderson1990cognitive}.  How do humans learn in simple and in complex systems?  How efficiently do they explore the space of possible beliefs and how closely is the direction of exploration tied to experience?  Bayesian approaches of learning would not be feasible for learning in realistically complex systems, given recall constraints and they are not defined in cases where initial beliefs are such that they put zero weight on some possible outcomes.  

Our work presents new experimental results on the rate at which people learn in more or less complex environments. We find that learning rates are much slower than they would be if learners were Bayesians, as had been proposed in older economic theories \cite{Boyer84, Prescott72, Rothschild74, McLennan84, Mirman84, Easley89, Kiefer89}. We also find that the learning rate is slightly higher when people build models of systems that are less complex and that even if the rates were identical accross levels of complexity, accuracy is always higher when the system is structurally simpler because initial models are closer aligned with reality.  

When searching for solutions to outstanding problems, in this case the formation of coherant belief structures that explain experience, humans must come up with innovative solutions.  We show that their strategies look a lot like a class of random search processes in which complete random guessing is strategically combined with the consolidation of past and current experience. We show how people shape their beliefs, starting with complete guesswork through a process that can be accurately modeled as a L'evy random search process, which involves both synthesis of current knowledge (mental exploitation) and out-of-the box mental exploration (see Figure \ref{fig:schematic}). Such random search processes are ubiquitous across the life sciences, in cognitive science, computer science  and artificial intelligence. We then measure how this process leads to convergence, albeit slow, to the correct stochastic solution.

For some time now, cognitive scientists have posed that modeling people's beliefs about causation in the material world provides an effective explanatory framework for how we process information and learn from these observations \cite{Griffiths2008}.  Probabilistic causes and effects, in other words, make up an important construct we seem to make use of when we seek to understand our world.  Bayes Nets are well-defined mathematical objects corresponding to an intuitive representation of causal beliefs.  They are easy to manipulate, do inference with and use as a means to compare different beliefs.  The kinds of Bayes Nets that have been allowed for in experimental settings up to this point, however, have been extremely simplistic and there has been an unanswered call for learning experiments with increased complexity (Griffiths?).  Our work is to our knowledge the first answer to this call.  

The data used in this work come from an experiment that we ran using an experimental platform designed to measure how participants form and update causal beliefs in more or less complex settings.  In the experiment, participants see a data stream: data about multiple binary variables in a system, which take on the value High or Low. For example, one variable could be a stock price and there may be a number of variables that explain its variation or are explained by its variation in turn.  A visual interface allows the subjects to draw a causal model of the system. The platform then computes predictions on the basis of the participant's causal model and makes those prediction visible to the participant. The participant then makes bets using these predictions. In doing so, the participant gets feedback on the accuracy of her beliefs, and can update her beliefs by modifying the causal model. The platform thus allows us to observe beliefs and learning in a very precise and controlled way.

As can be seen in Figure \ref{fig:decay}, the distance between a person's evolving belief system about some physical system and the long-run absorbing distribution, which we interpret as a performance score, decays with time in a very specific way:

\begin{equation}
\label{ }
d(belief_{i, t}, absorbing distribution) = \alpha*t^{-\beta_i}
\end{equation}



The functional form of human learning in the context of our experiment, then, is the same as for the Bayesian learner, but the decay rate, $\beta$, of a human learner is much smaller than that of a Bayesian. 


