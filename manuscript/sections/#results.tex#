\section{Experimental Results}

\subsection{Return to previously visited sites}
In both treatments of the Bayesian network reverse engineering experiment \cite{}, participants relied heavily on their memory as they returned to previous model iterations following a power law (see Figure \ref{fig:2}A and SI \ref{solutionspace_partitions} for details on solution space partitionning).\\

Return to previous mental structures : $\mathrm{pdf}(V_r) \sim {V_r}^{- \gamma -1}$, with $\gamma_{simple} = 1.6(1)$ and $\gamma_{complex} = 1.5(1)$ $\rightarrow$ tendency to return to previously visited sites : This goes against the imperative to visit new sites (maximize $S_T$) in order to reduce $D_{min}$ (c.f. Figures \ref{fig:Dmin_vs_St}B and \ref{fig:Dmin_vs_St}C). Moreover, given the large number of sites [$10^{8}$ (resp. $10^{16}$) in the simple (resp. complex) case], it is remarkable that participants tend to return to exactly the same miniscule cognitive regions. This characterizes cognitive stickiness.


\begin{figure}[h!]
\begin{center}
%\includegraphics[width=16cm]{figures/Figure1.eps}
\caption{\footnotesize{{\bf A.} return to previously visited sites : $\mathrm{pdf}(V_r) \sim {V_r}^{- \gamma -1}$, with $\gamma_{simple} = 1.6(1)$ and $\gamma_{complex} = 1.5(1)$ $\rightarrow$ tendency to return to previously visited sites : This goes against the imperative to visit new sites (maximize $S_T$) in order to reduce $D_{min}$ (c.f. Figures \ref{fig:Dmin_vs_St}B and \ref{fig:Dmin_vs_St}C). Moreover, given the large number of sites [$10^{8}$ (resp. $10^{16}$) in the simple (resp. complex) case], it is remarkable that participants tend to return to exactly the same (tiny) spots. This suggest some stickiness of memory. {\bf B.} Influence of a model proposed at time in subsequent models. The influence is computed as 1/distance between the focal model and subsequent models. On average over all participants in each treatment, influence $I$ decays as $I \sim t^{-\chi}$ with $\chi = 0.48(2)$ ($p < 0.01$ and $R > 0.32$). This result shows that memory is a long memory process, with implications for the convergence to. {\bf C.} Observed versus predicted mean square displacement ?.}}
\label{fig:3}
\end{center}
\end{figure}

\subsection{Waiting times \& Long-Memory Process}

{\bf [waiting times? ]}

Memory is also exhibited in the recombination of multiple previously explored models into new ones. We find that the influence of a focal model on subsequent models. On average over all participants in each treatment, influence $I$ decays as $I \sim t^{-\chi}$ with $\chi = 0.48(2)$ ($p < 0.01$ and $R > 0.32$)  (Figure \ref{fig:2}B). Because the time decay is power law with exponent $\chi < 1$, on average, models proposed in the past never vanish from memory, and may be reused (resp. inspired from). Assuming that {\bf [As ?]} the rate of proposed models remains roughly constant  overtime (see SI \ref{}?), as time passes, it gets statistically harder to come up with new solutions (``because minds gets polluted by past experience"). 

\subsection{Mean Square Displacement}
With the reported displacement and waiting time functions (c.f., Figures \ref{fig:pdfs}A and \ref{fig:pdfs}B), the diffusion process is supposed to be ballistic, and mean square displacement (MSD) should increase as a result. For humans searching a unique solution in a large problem space, this is not the case, and MSD decays as $\sim t^{\mu}$ with $\mu_{simple} =-0.23(2)$ and $\mu_{complex} =- 0.26(1)$ showing a slow convergence (on the contrary of a normal L\'evy flight / CTRW, which is characterized by respectively diffusion $\mu_{Levy} = 1$ or super-diffusion $\mu_{CTRW} = \beta$ \cite{21,23}). 

This suggests additional constraints, such as periodic return to previously visited problem sub-spaces, or new solutions that integrate information from previously proposed solutions following an alternation of {\it exploration} and {\it exploitation} mechanisms \cite{march1991exploration}. The former may be related to a kind of {\it preferential return} in the physical space \cite{song2010modelling} and to some extent to some hardwired instinct to return to previously visited food spots with hope that resource renewal has occurred \cite{}. The {\it exploration} vs. {\it exploitation} hypothesis is more related to how humans deal with complicated abstract (non-tangible) problems, such as innovation \cite{march1991exploration}.

\subsection{Recombination of previously proposed solutions}

{\bf [Small exponent shows relative predominance of long-term memory over short-term memory ? One must factor human timing either as an external factor or as part of brain processing (rather the latter if possible)]}

Recombination of past proposed models accounts for XX\% of moves, and yields on average  YY\% performance. It is equivalent to searching within the already explored solution envelop.


\subsection{Exploration of solutions beyond the convex hull}
We are interested in proposed models, which go beyond the solution space (convex hull) known at iteration $i$. If twice $\langle D_{jsd,i} \rangle$, the average distance of solution $j$ compared to all previous solutions $i < j$, is larger than $ max(D_{jsd})$ the maximum distance between 2 solutions in $iterations = \{1,...,i\}$, then the new proposed model {\it {\bf incorporates information, which is not a combination of previously explored models}} and contributes to the enlargement of the currently known solution space. To measure how much a proposed model is explorative, versus integrating knowledge from previously proposed models, we define the ratio:

\begin{equation}
EE_{j} = \frac{2 \langle D_{jsd,j} \rangle}{max(D_{jsd,1,...,j})}.
\end{equation}

For $EE < 1$, the individual is choosing a solution within the boundaries of the convex hull of already explored solutions. For $EE \approx 1$, the proposed solution is on -- or close to -- the boundaries, and for $EE > 1$, the proposed solution is outside the convex hull, and therefore explorative. The larger $EE$, the more explorative the proposed solution. For both 3-node and 4-node BN treatments,  the number of explorative moves is over all people much larger than exploitative ones :  $5416$ vs. $1873$ (resp. $6260$ vs. $4689$). {\bf However, with time ...}



\section{Implications of Memory and Exploration on Performance}
Individuals deploy {\it exploration}, {\it exploitation}, and {\it return} strategies in order to get closer to the solution (i.e., the true model). Each move may lead to improvement (i.e., reduction of distance to the true model) or, on the contrary, to dis-improvement. 

We find that displacement has an influence on improvement. As one could surmise, small displacements can only bring marginal improvement, while larger displacements bring more opportunities for improvement, yet at the cost of increased volatility. Large displacements ($\Delta r > 0.2$) bring rather negative performance. Figure \ref{fig:vs_dr} shows the evolution of the distance to the true model $D$ as a function of displacement $\Delta r$. The distance scales as $D \sim {\Delta r}^{\mu}$ with $\mu_{simple} = 0.88(1)$ [resp. $\mu_{complex} = 082(2)$]. For $\Delta r > 0.2$, $D$ becomes quickly highly uncertain, but rather positive, reflecting the {\it cost} of the making ``wild"displacements. 

There is no clear relation between time spent on taking the decision to move and performance {\bf [show $\Delta D_{jsd}$ versus $\Delta t$] if relevant: question here is whether waiting leads to better results.}.

However, the decision to make a larger displacement takes more time. For $\Delta r < 0.2$, the waiting time before a displacement decision is made scales as $\Delta t \sim \Delta r^{\gamma}$ with $\gamma_{simple} = 0.11(1)$ [resp. $\gamma_{complex} = 0.13(1)$]. For $\Delta r > 0.2$, the waiting time before a displacement decision is taken get disproportionally long (up to tens of seconds on average for displacement of 0.7 (i.e., $\approx 25\%$ of the maximum displacement distance). On both panels, blue and red areas show the 25th percentile confidence intervals.


{\bf [show $\Delta D_{jsd}$ versus Explore and Exploit]}

Influence of ``explorative" solutions versus ``exploitative" solutions ?

Influence of moves than improve score rather than those that don't ?


Minimum Euclidian distance $D_{min}$ (between the best model and the true model) exhibits a scaling as a function of the number of distinct sites visited $S_{T}$. $D_{min} \sim S_{T}^{\gamma}$ with resp. $\gamma_{simple} = -0.20(4)$ and $\gamma_{complex} = - 0.13(3)$. {\bf C.} The number of visited sites over time $S_t$ is a linear function of time $t$. Hence, the number of distinct visited sites is a predictor of the minimum distance $D_{min}$ achieved. The result also holds for average distance





%\section{Results (old)}
%
%Figure \ref{fig:schematic} shows the three possible choices made by individuals:
%
%\begin{itemize}
%  \item {\bf explore} : in that case, the individual will search for new solutions outside the convex hull of the currently known solution space (i.e., the (hyper-)surface defined by all already visited solutions. 
%  \item {\bf exploit} : individuals {\it remix} $p < n$ already visited solutions into the $n$-th solution.
%  \item {\bf return} : with some probability an individual may return to a previously visited site. 
%\end{itemize}
%
%
%
%%\subsection{$\Delta$ score as a function of displacement}
%%
%%Figure \ref{fig:vs_dr}A: Evolution of the distance to the true model $D$ as a function of displacement $\Delta r$. The distance scales as $D \sim {\Delta r}^{\mu}$ with $\mu_{simple} = 0.88(1)$ [resp. $\mu_{complex} = 082(2)$]. For $\Delta r > 0.2$, $D$ becomes quickly highly uncertain, but rather positive, reflecting the {\it cost} of the making ``wild"displacements. 
%%
%%\subsection{Waiting Time as a function of displacement}
%%
%%Figure \ref{fig:vs_dr}B: For $\Delta r < 0.2$, the waiting time before a displacement decision is made scales as $\Delta t \sim \Delta r^{\gamma}$ with $\gamma_{simple} = 0.11(1)$ [resp. $\gamma_{complex} = 0.13(1)$]. For $\Delta r > 0.2$, the waiting time before a displacement decision is taken get disproportionally long (up to tens of seconds on average for displacement of 0.7 (i.e., $\approx 25\%$ of the maximum displacement distance). 
%
%
%
%\subsection{Dynamics Formulation of Exploration/Exploitation}
%
%Now, one may consider that making one ``bad move" (with arguably large displacement) may not be immediately beneficial, but rather help on the long term, through a process of exploration (with no yield), followed by some efficient exploitation (when improvements are performed).
%
%For that, one may consider how past solutions influence newer, and how this promotes performance, or not. A special attention should be paid to (i) exploration and (ii) return. In the latter case, we shall try to determine whether returning is the acknowledgement of a dead-end (in that case return should bring sustainable improvement), or whether it is less rational (e.g., some kind of cognitive stickiness).
%
%To capture the exogenous contribution of new solution exploration and the endogenous contributions of exploitation of previous solutions.
%
%{\bf [here, explain why a DISCRETE Process is relevant: One could think of discrete neural net firing, which translates into an intensity of activity. Another explanation is more far-fetched by saying that Hawkes processes capture well human activity, which in turn stems from human intelligence and cognition.]}
%
%The dynamics of exploration versus exploitation can be captured together by a generic cascade process, which is well described by the excited  Hawkes conditional Poisson process \cite{hawkes1974acluster}.
%The Hawkes process typically captures well a variety of social dynamics involving complex human interactions such as online viral meme propagation \cite{crane2008}, gangs and crime in large American cities \cite{mohler2011}, cyber crime \cite{baldwin2012} and financial contagion \cite{ait-sahalia2010,filiminov2012,filiminov2014}.  
%The Hawkes process is defined by the intensity $I(t)$ of events (commits) given by
%\be
%I(t)= \lambda(t) + \sum_{i | t_t<t}  f_i \phi(t-t_i)~,
%\label{jruym}
%\ee
%where $\{t_i, i=1, 2, ...\}$ are the timestamps of past proposed solutions, $\lambda(t)$ is the spontaneous
%exogenous generation of new solutions (i.e., exploration), $f_{i}$ is the 
%fertility of solution $i$ that quantifies the number of offsprings (of first generation)
%that it can potentially trigger directly, and $ \phi(t-t_i)$ is the memory kernel, whose
%integral is normalized to $1$, which weights how 
%much past solutions influence future ones. $\phi$ relates to the decay of influence $I$ as $I \sim t^{-\chi}$ with $\chi = 0.48(2)$ ($p < 0.01$ and $R > 0.32$). This result shows a long memory process (see Figure \ref{fig:memory}), with implications for the convergence to the solution (see Figure \ref{fig:vs_dr}A). 
%
%
%%typically reflects how tasks are prioritized and performed by individuals according to a rational economy where time is a non storable resource \cite{maillart2011}. 
%Expression (\ref{jruym}) expresses that the current solution elaborated between $t$ and $t+dt$
%results from two sources: (i) an exogenous source $\lambda(t) dt$ representing exploration not related
%to previous solutions; (ii) an endogenous term represented by the sum over all past solutions that were 
%made prior to $t$, and which are susceptible to trigger future solutions (i.e., exploitation). 
%
%The Hawkes model is the simplest conditional Poisson process that combines both exogeneity and
%endogeneity. The class of Hawkes models can be mapped onto the general class of branching processes \cite{daley2007}. 
%The statistical average fertility $\langle f_i \rangle$ defines the branching ratio $n$, which is the key
%parameter. For $n<1$, $n=1$ and $n>1$, the process is respectively sub-critical, critical and super-critical \cite{helmstetter2002subcritical,helmstetter2003}.
%
%A schematic representation of the process at work is shown on Figure \ref{fig:schematic_remix}
%
%Figure \ref{fig:memory} : Influence of a model proposed at time in subsequent models. The influence is computed as $1/D$ between the focal model and subsequent models. If we consider a network with focal node $i$ (i.e., the newest solution) with edges of some length connecting to other nodes (i.e., past proposed solutions), then influence of the latter on the former, may be considered as weighted edges, with edge weight $= 1/D_{i,j}$ .
%
%
%
%In a nutshell, there are 2 somehow distinct discrete processes:
%
%\begin{itemize}
%  \item the discrete process of proposing a new solution: the waiting times between two propositions may relate to some information processing time by the mind.
%  \item the discrete process of blending exploration of new solutions with the exploitation of past proposed solutions: This blend may be seen as past solutions ``firing" repeatedly in the mind. The repeated firing shall translate into an intensity. This intensity is assumed to be proportional to the distance between the focal solution $i$ and past solutions $j$. In other words, the more a past solution $j$ ``fires" in the mind, the more likely the new solution $i$ will be close to  $j$. One may assume that firing occurs as the individual prepares her next solution, i.e., between solutions $i-1$ and $i$. \end{itemize}
%
%Memory firing 
%
%
%\subsection{Good or bad firing ? Renewal ? Evolutionary pressure?}
%Firing of past solutions may be positive or negative, depending on the balance between keeping in mind good past solutions (i.e., closer to the true solutions), expansion of the solution space (i.e., exploration), and renewal (i.e., leaving behind / forgetting past unfruitful solutions. 
%
%{\bf [We shall analyze this process and try to predict good versus bad performer in that framework.]}
%
%
%\subsection{Brain Processing Time \& Economics of Time}
%One cannot observe what's going on during these thinking moments, i.e., between two moves. We are left wondering. In foraging, the $\Delta t$ may be connected to travel time, or the time needed to consume resources (interestingly consuming these resources without depleting them completely, apparently). Here, the resource to consume is ``brain processing time", with an overall limited time budget. So individuals must strike a balance between spending time thinking and testing new solutions.  Figures \ref{fig:vs_dr}A and \ref{fig:vs_dr}B suggest that making big -- more risky/uncertain -- moves take more brain processing. 
%
%This may relate to Intertemporal choice ? $\rightarrow$ discounted utility / decision theory? It's somehow different because here we examine the process of making a choice: one may see the problem from two possible perspectives : 
%
%\begin{enumerate}
%  \item As individuals delay action, more reshuffling (renewal) occurs, and hence it is more likely that displacement may be greater.
%  \item Or on the contrary, the agent has for objective to make a large displacement, and she will spend more time looking strategically for a meaningful move (i.e., a non-trivial one that brings together exploration, exploitation and performance).
%\end{enumerate}
%
%{\bf [One may ask whether one or the other apply here ? Can we test either hypothesis ?]}
%
%
%\paragraph{Simple Economics of Time}
%Now one may think of a simple model of time efficiency. The simple question here is the following: shall an agent make multiple small moves with little expected incremental performance (i.e., Distance reduction) or on the contrary make less moves, yet with more displacement and hence, more expected performance (in less increments).
%
%On the one hand, we know that $\Delta D_{jsd} \sim - \Delta r ^{\mu}$ with $\mu \approx 0.85$ for $\Delta r < 0.2$, on the other hand, $\Delta t \sim \Delta r^{\gamma}$ with $\gamma \approx 0.12$, hence $\Delta r \sim \Delta t^{1/\gamma}$ also for $\Delta r < 0.2$.
%
%Therefore, 
%
%\begin{equation}
%\Delta D_{jsd} \sim -  \Delta t^{\mu / \gamma} \approx \Delta t^{7.1}~~with~~\Delta r < 0.2~(resp.~\Delta t  < 3)
%\end{equation}
%
%We want to maximize $\sum_{i=1}^{N} \Delta D_{jsd}$, with $N$ being the number of trials, which we want to maximize. Because $\Delta t$ diverges and $\Delta D_{jsd}$ get positive (dis-performance) when $\Delta r > 0.2$, we have the additional constraint $\Delta t  < 3$ seconds. Actually, the best strategy seems to be waiting for around 3 seconds. {\bf [it may look a bit naive $\rightarrow$ may be necessary to enrich the mechanism with e.g., some stochasticity]}.
%
%
%\begin{itemize}
%  \item {\bf [one may want to check the following prediction: those who wait close to 3 seconds, should perform better. If the prediction is verified, then what remix arrangement(s) occur?]}
%  \item {\bf [try to explain why beyond $\Delta r > 0.2$ performance get uncertain and on average worse]}
%\end{itemize}
%
%
%
%
%



%
%
%\subsection{Exploration}
%The exploration process involves visiting new sites and ensuring an efficient covering the problem space. We observe that the exploration pattern alternates local search and long-range jumps. This strategy is reminiscent of food (resp. resource) search {\it L\'evy flight} strategies followed by animals \cite{viswanathan1996levy,ramos2004levy,reynolds2007displaced} and by hunter-gatherers \cite{gonzalez2008understanding,song2010modelling,rhee2011levy}, through random search alternating short- and long-distance jumps. 
%
%The long jumps reduce the chance that same sites get visited multiple times, while clusters of small jumps help explore locally.
%
%The distribution of jump sizes $\Delta r$ is given by, 
%
%\begin{equation}
%P(R > \Delta r) \sim |\Delta r|^{-\alpha}, ~~\mathrm{with}~~\alpha = 0.6().
%\label{eq:jump_sizes}
%\end{equation}
%
%Since $\alpha = 0.6 < 1$ ($\alpha$ was obtained by maximum likelihood estimation with confidence interval obtained by bootstrapping \cite{maillart,maillart,clauset}), see Figure \ref{fig:jump_sizes}) the process is super-diffusive in space, hence promoting a broad exploration of the problem space (since  $\alpha < 1$ the first (average) and second (variance) statistical moments diverge with realizations $n \rightarrow \infty$, hence ensuring exploration, bounded only by the solution space. The bounded solution space is reflected by the truncation $P(R> \Delta r) = 0$ for $r > 1$ (the theoretical limit is ${\Delta r}_{max} = \sqrt{8} \approx 2.8$).
%
%The super-diffusive process in space is counter balanced by the waiting times between L\'evy flight moves. The distribution of waiting times follows a power law distribution given by,
%
%\begin{equation}
%P(T > \Delta t) \sim |\Delta t|^{-\beta}, ~~with~~\beta \approx 0.5,
%\label{eq:waiting_times}
%\end{equation}
%
%with a change of regime for $\Delta t  > 100$ seconds ($\alpha \approx 1.5$ for $\Delta t > 100$) see Figure \ref{fig:waiting_time}). ($\alpha$ was obtained by adaptive kernel estimation \cite{}. This method (fitting the pdf instead of the cdf) is required to account for the change of regime).
%
%
%The combination of jump sizes (\ref{eq:jump_sizes}) and waiting times (\ref{eq:waiting_times}), qualifies a continuous time random walk (CTRW), which diffusion (mean square displacement MSD) can be predicted to be $\mu = 2\beta / \alpha \approx 1.6$. Since $1< \mu < 2$, the CTRW should be super-diffusive, yet not ballistic (since $\mu > 1$), in case $\Delta r$ and $\Delta t$ are uncorrelated.
%
%However, we observe that $MSD \sim t^{\overline{\mu}}$ with $\overline{\mu} = 0.35 < \mu = 1.6$.
% 
%Actually, we find that $\Delta t$ and $\Delta r$ exhibit a scaling dependency (see. Figure \ref{fig:corr_dt_vs_dr}),
%
%% $\Delta r \approx {\Delta t}^{\gamma}$ with $\gamma_{simple} = 0.23(2)$ ($p < 0.01$, $r > 0.2$ in the {\it simple} treatment; Spearman $\rho = 0.33$ $p < 0.01$, $\gamma_{complex} = 0.15(3)$ with $p < 0.01$, $r > 0.13$, Spearman $\rho = 0.53$ $p < 0.01$).
%
%{\bf [$\rightarrow$ how this scaling relation influence diffusion? it should influence negatively, but the right derivation must be worked out]}
%
%\paragraph{Montroll-Weiss Equation}
%
%\be
%\rho(k,s) = \rho(k,0) \frac{1 - \phi(s)}{s} \frac{1}{1 -  \Phi(s) \Psi(k)}
%\ee
%
%
%
%
%
%\subsection{Return to Previous Sites}
%$\rightarrow$ The scaling dependency between $\Delta r$ and $\Delta t$ does not explain all the the discrepancies observed between $\overline{\mu}$ and $\mu$.
%
%We suspect that people ``return" to previously visited site, in a way that is also consistent with observed food hunting strategies \cite{}
%
%To determine return to previous solution sites, we consider a grid partition of the solution space in 4 (resp. 16) dimensional space, with $\bar{s_k} = \{0.01,0.02, .., 1\}$ (At this resolution level, the solution space is thus $10^{9}$ sites for the simple Bayesian network (resp. $10^{16}$ sites in the complex treatment). Distance between two sites is measured as $\Delta r_{i,j} = \| \mathbf{s}_j - \mathbf{s}_i \| = \sqrt{\sum_{k=1}^{k=8} (\mathbf{s}_{j,k} - \mathbf{s}_{i,k})^{2}}$ for $k = \{1,..,8\}$ (resp. $k = \{1,..,16\})$. With a $0.01$ grid resolution the maximum error between a proposed model and the true solution is $\epsilon_{\Delta r} = 0.01 \times \sqrt{2} \approx 2.8\%$.\\
%
%We observe that the number of new locations visited over time follows
%
%\begin{equation}
%S(t)  \sim t^{\mu},
%\end{equation}
%
%with $\mu \approx 0.7$. This is at odds with typical random search strategies involving long range {\it flights}: for L\'evy flights $\mu = 1$ \cite{} and for Continuous Time Random Walk $\mu = \beta$ \cite{}). 
%
%Since $\Delta r$ and $\Delta t$ are un-correlated, the sub-linear scaling visitation of new sites cannot be attributed to increased delays between jumps over time [i.e., $P(\Delta t|t) = P(\Delta t)$]. Similarly, $P(\Delta r|r(t)) = P(\Delta r)$. The only explanation for the convex nature of $S$ is the return to previously visited sites. Figure \ref{} shows the probability distribution of site visitation  $V$, which best described by a power law distribution
%
%\begin{equation}
%P(V > v) \sim v^{-\gamma}
%\end{equation}
%
%with $\gamma \approx 2$. In contrast, the probability $V$ of visitation is expected to be asymptotically ($t \rightarrow \infty$) uniform (P(V) ~ const.)\\
%
%
%\subsection{Convergence}
%We find that the Euclidian distance between a proposed Bayesian network (the {\it model} thereafter) and the true Bayesian network (the {\it solution}), decays on average as, 
%
%\begin{equation}
%\langle D(t) \rangle  \sim t^{-\nu},
%\end{equation}
%
%with $\nu_{simple} = 0.149(2)$ and $\nu_{complex} = 0.162(2)$ ( $p < 0.01$, $r^2 > 0.7$) (see Figure \ref{fig:decay}). Since $\nu \ll 1$, the distance between the model and the solution converges extremely slowly. By extrapolation, it would take 7 days to ensure $\langle D \rangle < 0.1$ [one realization (i.e., one participant) may however reach that threshold, but it remains unclear whether if it would be a matter of chance or the result of a better search strategy]. Given the size of the problem space (see above), and the solution sparsity in this space (there is indeed only one possible solution), one shall expect a slow convergence on average. 
%
%\subsection{Positive/Negative implications of return to previously visited sites}
%
%return to previously visited sites may be helpful to return to previously better solution
%
%
%\subsection{Exploration versus Exploitation}
%
%
%
%\subsection{Implications of waiting times with change of regime: cascades ?}
%
%


%
%\subsection{Ultra Slow Diffusion / Power Law Decay}
%
%The Continuous Time Random Walk Model (CTRW) of solution search predicts that the mean square displacement (MSD) of a person's model from the stochastic properties of the real system asymptotically follows $\langle \Delta x^2 (t) \rangle \sim t^{\nu}$ with $\nu = 2\beta /\alpha$. 
%
%\begin{equation}
%\label{power_law_decay}
%CD(t) = C \cdot t^{-\alpha},
%\end{equation}
%
%where we estimate from our experimental data that $\alpha = 0.09$ and we find that $C$ is a constant which depends on the system that people make their inferences about; in our case, the $simple$ and $complex$ treatments. 
%
%\begin{equation}
%\label{ultraslowdiffusion}
%S(t) = 1 - CD(t) = 1- C \cdot t^{-\alpha},
%\end{equation}
%
%
%\subsubsection{Stepwise Jumps}
%
%\begin{equation}
%P(R > \Delta r) \sim |\Delta r|^{-\alpha}, ~~with~~\alpha \approx 0.1,
%\end{equation}
%
%jump size $\Delta r$ Figure \ref{fig:jump_sizes}


%\section{Theory and Findings}
%
%\subsection{Ultra Slow Diffusion / Power Law Decay}
%
%The Continuous Time Random Walk Model (CTRW) of solution search predicts that the mean square displacement (MSD) of a person's model from the stochastic properties of the real system asymptotically follows $\langle \Delta x^2 (t) \rangle \sim t^{\nu}$ with $\nu = 2\beta /\alpha$. 
%
%\begin{equation}
%\label{power_law_decay}
%CD(t) = C \cdot t^{-\alpha},
%\end{equation}
%
%where we estimate from our experimental data that $\alpha = 0.09$ and we find that $C$ is a constant which depends on the system that people make their inferences about; in our case, the $simple$ and $complex$ treatments. 
%
%\begin{equation}
%\label{ultraslowdiffusion}
%S(t) = 1 - CD(t) = 1- C \cdot t^{-\alpha},
%\end{equation}
%
%
%\subsubsection{Stepwise Jumps}
%
%\begin{equation}
%P(R > \Delta r) \sim |\Delta r|^{-\alpha}, ~~with~~\alpha \approx 0.1,
%\end{equation}
%
%jump size $\Delta r$ Figure \ref{fig:jump_sizes}

