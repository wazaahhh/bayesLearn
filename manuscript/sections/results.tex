\section{Results}
When humans solve hard-problems, we find that limits on cognition seem to constrain the continuous-time random walk (CTRW) process, the patterns of which are widely found in nature. 


With the reported displacement and waiting time functions (c.f., Figures \ref{fig:pdfs}A and \ref{fig:pdfs}B), models predict that the diffusion process is ballistic, and mean square displacement (MSD) increases as a result. For our laboratory participants, searching for a unique solution in a large problem space, this was not the case, and MSD decays as $\sim t^{\mu}$ with $\mu_{simple} =-0.23(2)$ and $\mu_{complex} =- 0.26(1)$ exhibited a slow convergence (compared to the L\'evy flight / CTRW, which is optimal and characterized by $1<\mu_{Levy} /leq 3$ or super-diffusion $\mu_{CTRW} = \beta$ \cite{21,23}). 
%I don't understand $\mu_{CTRW} = \beta$ ? 

These findings suggest additional constraints, such as periodic return to previously visited problem sub-spaces, or integration of information from previously proposed solutions to synthesize new solutions following an alternation of {\it exploration} and {\it exploitation} mechanisms \cite{march1991exploration}. The former may be related to a kind of {\it preferential return} in the physical space \cite{song2010modelling} and may to some extent be a hardwired instinct to return to previously visited food spots in hope that resource renewal has occurred \cite{}. The {\it exploration} vs. {\it exploitation} hypothesis has an observable implication--the patterns that result from that behavior--of human constraints with respect to complex abstract (non-tangible) problems and innovation \cite{march1991exploration}.  Evolutionary optimality, then, seems to constrain modern optimality: our ability to explore without returning to previous locations in the abstract thought space.  

Figure \ref{fig:schematic} shows the three possible choices made by individuals:

\begin{itemize}
  \item {\bf explore} : when a participant explores, she will search for new solutions outside the convex hull of the currently known solution space (i.e., the (hyper-)surface defined by all already visited solutions. 
  \item {\bf exploit} : participants {\it remix} $p < n$ already visited solutions to arrive at their $n$-th solution.
  \item {\bf return} : with some probability an individual may return to a previously visited site. 
\end{itemize}

\paragraph{Exploration / Exploitation}


\begin{equation}
EE_{j} = \frac{2 \langle D_{jsd,j} \rangle}{max(D_{jsd})}  
\end{equation}

with $\langle D_{jsd,i} \rangle$ the average distance of solution $i$ compared to all previous solutions $j < i$, and $ max(D_{jsd})$ the maximum distance between 2 solutions. For $EE < 1$, the individual is choosing a solution within the boundaries of the convex hull of already explored solutions. For $EE \approx 1$, the proposed solution is on -- or close to -- the boundaries, and for $EE > 1$, the proposed solution is outside the convex hull, and therefore explorative. The larger $EE$, the more explorative the proposed solution. For both 3-node and 4-node BN treatments,  the number of explorative moves is over all people much larger than exploitative ones :  $5416$ vs. $1873$ (resp. $6260$ vs. $4689$). 

Minimum Euclidian distance $D_{min}$--between the best model thus far and the correct model (in the experimental setting the model that generated the data observed by our participants is the ``correct model'')--exhibits a scaling as a function of the number of distinct sites visited $S_{T}$. $D_{min} \sim S_{T}^{\gamma}$ with resp. $\gamma_{simple} = -0.20(4)$ and $\gamma_{complex} = - 0.13(3)$. {\bf C.} The number of visited sites over time $S_t$ is a linear function of time $t$. Hence, the number of distinct visited sites is a predictor of the minimum distance $D_{min}$ achieved. The result also holds for average distance.

\paragraph{Return}
Figure \ref{fig:pdf_return}: Return to previously visited sites : $\mathrm{pdf}(V_r) \sim {V_r}^{- \gamma -1}$, with $\gamma_{simple} = 1.6(1)$ and $\gamma_{complex} = 1.5(1)$ $\rightarrow$ tendency to return to previously visited sites : This goes against the imperative to visit new sites (maximize $S_T$) in order to reduce $D_{min}$ (c.f. Figures \ref{fig:Dmin_vs_St}B and \ref{fig:Dmin_vs_St}C). Moreover, given the large number of sites [$10^{8}$ (resp. $10^{16}$) in the simple (resp. complex) case], it is remarkable that participants tend to return to exactly the same (tiny) spots. This suggest some stickiness of memory.


%\subsection{$\Delta$ score as a function of displacement}
%
%Figure \ref{fig:vs_dr}A: Evolution of the distance to the true model $D$ as a function of displacement $\Delta r$. The distance scales as $D \sim {\Delta r}^{\mu}$ with $\mu_{simple} = 0.88(1)$ [resp. $\mu_{complex} = 082(2)$]. For $\Delta r > 0.2$, $D$ becomes quickly highly uncertain, but rather positive, reflecting the {\it cost} of the making ``wild"displacements. 
%
%\subsection{Waiting Time as a function of displacement}
%
%Figure \ref{fig:vs_dr}B: For $\Delta r < 0.2$, the waiting time before a displacement decision is made scales as $\Delta t \sim \Delta r^{\gamma}$ with $\gamma_{simple} = 0.11(1)$ [resp. $\gamma_{complex} = 0.13(1)$]. For $\Delta r > 0.2$, the waiting time before a displacement decision is taken get disproportionally long (up to tens of seconds on average for displacement of 0.7 (i.e., $\approx 25\%$ of the maximum displacement distance). 


\subsection{good versus bad models}
Individuals deploy {\it exploration}, {\it exploitation}, and {\it return} strategies in order to get closer to the solution (i.e., the true model). Each move may lead to improvement (i.e., reduction of distance to the true model) or, on the contrary, to dis-improvement. 

We find that displacement has an influence on improvement. As one could surmise, small displacements can only bring marginal improvement, while larger displacements bring more opportunities for improvement, yet at the cost of increased volatility. Large displacements ($\Delta r > 0.2$) bring rather negative performance. Figure \ref{fig:vs_dr} shows the evolution of the distance to the true model $D$ as a function of displacement $\Delta r$. The distance scales as $D \sim {\Delta r}^{\mu}$ with $\mu_{simple} = 0.88(1)$ [resp. $\mu_{complex} = 082(2)$]. For $\Delta r > 0.2$, $D$ uncertainty steeply increases, reflecting the {\it cost} of making ``wild" displacements. 

While there is no clear relation between time spent between moves and performance {\bf [$\Delta D_{jsd}$ versus $\Delta t$]}, the decision to make a larger displacement takes more time. For $\Delta r < 0.2$, the waiting time before a displacement decision is made scales as $\Delta t \sim \Delta r^{\gamma}$ with $\gamma_{simple} = 0.11(1)$ [resp. $\gamma_{complex} = 0.13(1)$]. For $\Delta r > 0.2$, the waiting times before a displacement decision is taken get disproportionally long (up to tens of seconds on average for a displacement of 0.7 (i.e., $\approx 25\%$ of the maximum displacement distance). On both panels, blue and red areas show the 25th percentile confidence intervals.


{\bf [show $\Delta D_{jsd}$ versus Explore and Exploit]}

{\bf [$\rightarrow$  ``Good" or ``Bad" memory ?]}


Influence of ``explorative" solutions versus ``exploitative" solutions ?

Influence of moves that improve scores rather than those that don't ?



\subsection{Dynamics Formulation of Exploration/Exploitation}

One may consider that making one ``bad move" (with arguably large displacement) may not be immediately beneficial, but it may help in the long term through a process of exploration (with no yield), followed by some efficient exploitation during which improvements are performed via combination.

For that, we must consider how past solutions influence new ones, and how this effects performance. Special attention should be paid to (i) explorations and (ii) returns to previous solutions. We shall determine whether a return to a previous solution likely signals an acknowledgement of having reached a dead-end (in which case return should bring sustainable improvement), or whether such returns are likely less reasoned (e.g., the result of some kind of cognitive stickiness).

%To capture the exogenous contribution of new solution exploration and the endogenous contributions of exploitation of previous solutions. 

%{\bf [here, explain why a DISCRETE Process is relevant: One could think of discrete neural net firing, which translates into an intensity of activity. Another explanation is more far-fetched by saying that Hawkes processes capture well human activity, which in turn stems from human intelligence and cognition.]}

The dynamics of exploration versus exploitation can be captured together by a generic cascade process, which is well described by the excited  Hawkes conditional Poisson process \cite{hawkes1974acluster}.
The Hawkes process typically captures well a variety of social dynamics involving complex human interactions such as online viral meme propagation \cite{crane2008}, gangs and crime in large American cities \cite{mohler2011}, cyber crime \cite{baldwin2012} and financial contagion \cite{ait-sahalia2010,filiminov2012,filiminov2014}.  
The Hawkes process is defined by the intensity $I(t)$ of events (commits) given by
\be
I(t)= \lambda(t) + \sum_{i | t_t<t}  f_i \phi(t-t_i)~,
\label{jruym}
\ee
where $\{t_i, i=1, 2, ...\}$ are the timestamps of past proposed solutions, $\lambda(t)$ is the spontaneous
exogenous generation of new solutions (i.e., exploration), $f_{i}$ is the 
fertility of solution $i$ that quantifies the number of offsprings (of first generation)
that it can potentially trigger directly, and $ \phi(t-t_i)$ is the memory kernel, whose
integral is normalized to $1$, which weights how 
much past solutions influence future ones. $\phi$ relates to the decay of influence $I$ as $I \sim t^{-\chi}$ with $\chi = 0.48(2)$ ($p < 0.01$ and $R > 0.32$). This result shows a long memory process (see Figure \ref{fig:memory}), with implications for the convergence to the solution (see Figure \ref{fig:vs_dr}A). 


%typically reflects how tasks are prioritized and performed by individuals according to a rational economy where time is a non storable resource \cite{maillart2011}. 
Expression (\ref{jruym}) expresses that the current solution elaborated between $t$ and $t+dt$
results from two sources: (i) an exogenous source $\lambda(t) dt$ representing exploration not related
to previous solutions; (ii) an endogenous term represented by the sum over all past solutions that were 
made prior to $t$, and which are susceptible to trigger future solutions (i.e., exploitation). 

The Hawkes model is the simplest conditional Poisson process that combines both exogeneity and
endogeneity. The class of Hawkes models can be mapped onto the general class of branching processes \cite{daley2007}. 
The statistical average fertility $\langle f_i \rangle$ defines the branching ratio $n$, which is the key
parameter. For $n<1$, $n=1$ and $n>1$, the process is respectively sub-critical, critical and super-critical \cite{helmstetter2002subcritical,helmstetter2003}.

A schematic representation of the process at work is shown on Figure \ref{fig:schematic_remix}

Figure \ref{fig:memory} : Influence of a model proposed at time $t$ in subsequent models. The influence is computed as $1/D$ between the focal model and subsequent models. If we consider a network with focal node $i$ (i.e., the newest solution) with edges of some length connecting to other nodes (i.e., past proposed solutions), then influence of the latter on the former, may be considered as weighted edges, with edge weight $= 1/D_{i,j}$ .



In sum, there are two distinct discrete processes:

\begin{itemize}
  \item the discrete process of proposing a new solution: the waiting times between two propositions may relate to the time thatit takes the mind to process information.
  \item the discrete process of blending exploration of new solutions with the exploitation of past proposed solutions: This blend may be seen as past solutions ``firing" repeatedly in the mind. The repeated firing translates into intensity. This intensity is assumed to be proportional to the distance between the focal solution $i$ and past solutions $j$. In other words, the more a past solution $j$ ``fires" in the mind, the closer we predict the new solution $i$ to be to $j$. One may assume that firing occurs as the individual prepares her next solution, i.e., between solutions $i-1$ and $i$. \end{itemize}

Memory firing 


\subsection{Good or bad firing ? Renewal ? Evolutionary pressure?}
Firing of past solutions may be positive or negative, depending on the balance between keeping in mind good past solutions (i.e., closer to the true solutions), expansion of the solution space (i.e., exploration), and renewal i.e. forgetting the folly of past solutions. 

{\bf [We shall analyze this process and try to predict good versus bad performer in that framework.]}


\subsection{Brain Processing Time \& Economics of Time}
We cannot directly observe the cognitive processes that occur during the time that elapses between the times at which solutions are proposed. In foraging, $\Delta t$ denotes the time that it takes to travel between potential food sources and the time needed to consume the resources that are found (interestingly humans generally consume these resources without depleting them completely). Here, the resource to consume is the difference between participants actual payoffs and those that they could have achieved if they had used their ``brain processing time" differently (opportunity costs), with a finite and limited time budget. To be precise, monitory payoffs, that depend on the present quality of the current proposed model, occur every two minutes for a total of 30 minutes, after an initial 15 minutes of unpaid learning how the game is played. So individuals must strike a balance between spending time thinking and testing new solutions.  Figures \ref{fig:vs_dr}A and \ref{fig:vs_dr}B suggest that making big -- more risky/uncertain -- moves takes more brain processing, as a greater region of previous exploration puts more demand on one's ability to integrate. 

%This may relate to Intertemporal choice ? $\rightarrow$ discounted utility / decision theory? It's somehow different because 
Here we examine not the choices that participants make in uncertain and risky environments (or intertemporal choice) with known probabilities, as is common in economic models, but rather we examine the process of making a choice about guessing and learning unknown probabilities, with no assumptions about priors.  One may see the problem from two possible perspectives : 


\begin{enumerate}
  \item (Passive guessing, followed by testing) As individuals delay action, more cognitive reshuffling (renewal) occurs, and hence it is more likely that the displacement is greater.
  \item (Active thought and speculation, followed by strategic exploitation) Or on the contrary, the agent has the objective to make a large displacement, and she will spend more time looking strategically for a meaningful move (i.e., a non-trivial one that brings together exploration, exploitation and performance).
\end{enumerate}

{\bf [One may ask whether one or the other apply here ? Can we test either hypothesis ?]}


\paragraph{Simple Economics of Time}
We propose a simple model of time efficiency. The question here is the following: shall an agent make multiple small moves with little expected incremental performance (i.e., Distance reduction) or on the contrary make fewer moves, with larger displacements and hence more expected performance (in fewer increments)?

On the one hand, we know that $\Delta D_{jsd} \sim - \Delta r ^{\mu}$ with $\mu \approx 0.85$ for $\Delta r < 0.2$, on the other hand, $\Delta t \sim \Delta r^{\gamma}$ with $\gamma \approx 0.12$, hence $\Delta r \sim \Delta t^{1/\gamma}$ also for $\Delta r < 0.2$.

Therefore, 

\begin{equation}
\Delta D_{jsd} \sim -  \Delta t^{\mu / \gamma} \approx \Delta t^{7.1}~~with~~\Delta r < 0.2~(resp.~\Delta t  < 3)
\end{equation}

We want to maximize $\sum_{i=1}^{N} \Delta D_{jsd}$, with $N$ being the number of trials, which we want to maximize. Because $\Delta t$ diverges and $\Delta D_{jsd}$ becomes positive (dis-performance) when $\Delta r > 0.2$, we have the additional constraint $\Delta t  < 3$ seconds. The best strategy then seems to be to wait for approximately 3 seconds. {\bf [it may look a bit naive $\rightarrow$ may be necessary to enrich the mechanism with e.g., some stochasticity]}.


\begin{itemize}
  \item {\bf [one may want to check the following prediction: those who wait close to 3 seconds, should perform better. If the prediction is verified, then what remix arrangement(s) occur?]}
  \item {\bf [try to explain why beyond $\Delta r > 0.2$ performance get uncertain and on average worse]}
\end{itemize}








%
%
%\subsection{Exploration}
%The exploration process involves visiting new sites and ensuring an efficient covering the problem space. We observe that the exploration pattern alternates local search and long-range jumps. This strategy is reminiscent of food (resp. resource) search {\it L\'evy flight} strategies followed by animals \cite{viswanathan1996levy,ramos2004levy,reynolds2007displaced} and by hunter-gatherers \cite{gonzalez2008understanding,song2010modelling,rhee2011levy}, through random search alternating short- and long-distance jumps. 
%
%The long jumps reduce the chance that same sites get visited multiple times, while clusters of small jumps help explore locally.
%
%The distribution of jump sizes $\Delta r$ is given by, 
%
%\begin{equation}
%P(R > \Delta r) \sim |\Delta r|^{-\alpha}, ~~\mathrm{with}~~\alpha = 0.6().
%\label{eq:jump_sizes}
%\end{equation}
%
%Since $\alpha = 0.6 < 1$ ($\alpha$ was obtained by maximum likelihood estimation with confidence interval obtained by bootstrapping \cite{maillart,maillart,clauset}), see Figure \ref{fig:jump_sizes}) the process is super-diffusive in space, hence promoting a broad exploration of the problem space (since  $\alpha < 1$ the first (average) and second (variance) statistical moments diverge with realizations $n \rightarrow \infty$, hence ensuring exploration, bounded only by the solution space. The bounded solution space is reflected by the truncation $P(R> \Delta r) = 0$ for $r > 1$ (the theoretical limit is ${\Delta r}_{max} = \sqrt{8} \approx 2.8$).
%
%The super-diffusive process in space is counter balanced by the waiting times between L\'evy flight moves. The distribution of waiting times follows a power law distribution given by,
%
%\begin{equation}
%P(T > \Delta t) \sim |\Delta t|^{-\beta}, ~~with~~\beta \approx 0.5,
%\label{eq:waiting_times}
%\end{equation}
%
%with a change of regime for $\Delta t  > 100$ seconds ($\alpha \approx 1.5$ for $\Delta t > 100$) see Figure \ref{fig:waiting_time}). ($\alpha$ was obtained by adaptive kernel estimation \cite{}. This method (fitting the pdf instead of the cdf) is required to account for the change of regime).
%
%
%The combination of jump sizes (\ref{eq:jump_sizes}) and waiting times (\ref{eq:waiting_times}), qualifies a continuous time random walk (CTRW), which diffusion (mean square displacement MSD) can be predicted to be $\mu = 2\beta / \alpha \approx 1.6$. Since $1< \mu < 2$, the CTRW should be super-diffusive, yet not ballistic (since $\mu > 1$), in case $\Delta r$ and $\Delta t$ are uncorrelated.
%
%However, we observe that $MSD \sim t^{\overline{\mu}}$ with $\overline{\mu} = 0.35 < \mu = 1.6$.
% 
%Actually, we find that $\Delta t$ and $\Delta r$ exhibit a scaling dependency (see. Figure \ref{fig:corr_dt_vs_dr}),
%
%% $\Delta r \approx {\Delta t}^{\gamma}$ with $\gamma_{simple} = 0.23(2)$ ($p < 0.01$, $r > 0.2$ in the {\it simple} treatment; Spearman $\rho = 0.33$ $p < 0.01$, $\gamma_{complex} = 0.15(3)$ with $p < 0.01$, $r > 0.13$, Spearman $\rho = 0.53$ $p < 0.01$).
%
%{\bf [$\rightarrow$ how this scaling relation influence diffusion? it should influence negatively, but the right derivation must be worked out]}
%
%\paragraph{Montroll-Weiss Equation}
%
%\be
%\rho(k,s) = \rho(k,0) \frac{1 - \phi(s)}{s} \frac{1}{1 -  \Phi(s) \Psi(k)}
%\ee
%
%
%
%
%
%\subsection{Return to Previous Sites}
%$\rightarrow$ The scaling dependency between $\Delta r$ and $\Delta t$ does not explain all the the discrepancies observed between $\overline{\mu}$ and $\mu$.
%
%We suspect that people ``return" to previously visited site, in a way that is also consistent with observed food hunting strategies \cite{}
%
%To determine return to previous solution sites, we consider a grid partition of the solution space in 4 (resp. 16) dimensional space, with $\bar{s_k} = \{0.01,0.02, .., 1\}$ (At this resolution level, the solution space is thus $10^{9}$ sites for the simple Bayesian network (resp. $10^{16}$ sites in the complex treatment). Distance between two sites is measured as $\Delta r_{i,j} = \| \mathbf{s}_j - \mathbf{s}_i \| = \sqrt{\sum_{k=1}^{k=8} (\mathbf{s}_{j,k} - \mathbf{s}_{i,k})^{2}}$ for $k = \{1,..,8\}$ (resp. $k = \{1,..,16\})$. With a $0.01$ grid resolution the maximum error between a proposed model and the true solution is $\epsilon_{\Delta r} = 0.01 \times \sqrt{2} \approx 2.8\%$.\\
%
%We observe that the number of new locations visited over time follows
%
%\begin{equation}
%S(t)  \sim t^{\mu},
%\end{equation}
%
%with $\mu \approx 0.7$. This is at odds with typical random search strategies involving long range {\it flights}: for L\'evy flights $\mu = 1$ \cite{} and for Continuous Time Random Walk $\mu = \beta$ \cite{}). 
%
%Since $\Delta r$ and $\Delta t$ are un-correlated, the sub-linear scaling visitation of new sites cannot be attributed to increased delays between jumps over time [i.e., $P(\Delta t|t) = P(\Delta t)$]. Similarly, $P(\Delta r|r(t)) = P(\Delta r)$. The only explanation for the convex nature of $S$ is the return to previously visited sites. Figure \ref{} shows the probability distribution of site visitation  $V$, which best described by a power law distribution
%
%\begin{equation}
%P(V > v) \sim v^{-\gamma}
%\end{equation}
%
%with $\gamma \approx 2$. In contrast, the probability $V$ of visitation is expected to be asymptotically ($t \rightarrow \infty$) uniform (P(V) ~ const.)\\
%
%
%\subsection{Convergence}
%We find that the Euclidian distance between a proposed Bayesian network (the {\it model} thereafter) and the true Bayesian network (the {\it solution}), decays on average as, 
%
%\begin{equation}
%\langle D(t) \rangle  \sim t^{-\nu},
%\end{equation}
%
%with $\nu_{simple} = 0.149(2)$ and $\nu_{complex} = 0.162(2)$ ( $p < 0.01$, $r^2 > 0.7$) (see Figure \ref{fig:decay}). Since $\nu \ll 1$, the distance between the model and the solution converges extremely slowly. By extrapolation, it would take 7 days to ensure $\langle D \rangle < 0.1$ [one realization (i.e., one participant) may however reach that threshold, but it remains unclear whether if it would be a matter of chance or the result of a better search strategy]. Given the size of the problem space (see above), and the solution sparsity in this space (there is indeed only one possible solution), one shall expect a slow convergence on average. 
%
%\subsection{Positive/Negative implications of return to previously visited sites}
%
%return to previously visited sites may be helpful to return to previously better solution
%
%
%\subsection{Exploration versus Exploitation}
%
%
%
%\subsection{Implications of waiting times with change of regime: cascades ?}
%
%


%
%\subsection{Ultra Slow Diffusion / Power Law Decay}
%
%The Continuous Time Random Walk Model (CTRW) of solution search predicts that the mean square displacement (MSD) of a person's model from the stochastic properties of the real system asymptotically follows $\langle \Delta x^2 (t) \rangle \sim t^{\nu}$ with $\nu = 2\beta /\alpha$. 
%
%\begin{equation}
%\label{power_law_decay}
%CD(t) = C \cdot t^{-\alpha},
%\end{equation}
%
%where we estimate from our experimental data that $\alpha = 0.09$ and we find that $C$ is a constant which depends on the system that people make their inferences about; in our case, the $simple$ and $complex$ treatments. 
%
%\begin{equation}
%\label{ultraslowdiffusion}
%S(t) = 1 - CD(t) = 1- C \cdot t^{-\alpha},
%\end{equation}
%
%
%\subsubsection{Stepwise Jumps}
%
%\begin{equation}
%P(R > \Delta r) \sim |\Delta r|^{-\alpha}, ~~with~~\alpha \approx 0.1,
%\end{equation}
%
%jump size $\Delta r$ Figure \ref{fig:jump_sizes}


%\section{Theory and Findings}
%
%\subsection{Ultra Slow Diffusion / Power Law Decay}
%
%The Continuous Time Random Walk Model (CTRW) of solution search predicts that the mean square displacement (MSD) of a person's model from the stochastic properties of the real system asymptotically follows $\langle \Delta x^2 (t) \rangle \sim t^{\nu}$ with $\nu = 2\beta /\alpha$. 
%
%\begin{equation}
%\label{power_law_decay}
%CD(t) = C \cdot t^{-\alpha},
%\end{equation}
%
%where we estimate from our experimental data that $\alpha = 0.09$ and we find that $C$ is a constant which depends on the system that people make their inferences about; in our case, the $simple$ and $complex$ treatments. 
%
%\begin{equation}
%\label{ultraslowdiffusion}
%S(t) = 1 - CD(t) = 1- C \cdot t^{-\alpha},
%\end{equation}
%
%
%\subsubsection{Stepwise Jumps}
%
%\begin{equation}
%P(R > \Delta r) \sim |\Delta r|^{-\alpha}, ~~with~~\alpha \approx 0.1,
%\end{equation}
%
%jump size $\Delta r$ Figure \ref{fig:jump_sizes}

