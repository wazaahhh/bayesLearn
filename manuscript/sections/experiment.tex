\section{experiment}

The aim of the experiment originally was to assess the effect that the structural complexity of a system might have on 1) the distance between human beliefs and the actual system, as well as on 2) learning speed and 3) on the diversity of beliefs in groups of independent (non-communicating) observers of those systems. The results were that the complextity does effect distances and diversity, but these effects do not change over time, so that learning and the speed of homogenization of beliefs are not effected by complexity.  This last finding came as a surprise (Castner, forthcoming).  

The experiment was conducted at Columbia University's Social Science laboratory.  In total, 96 participants were asked to reverse engineer the structures and parameters that generated the data: one 3-node and one 4-node Bayesian Network (48 participants were assigned to each treatment). The {\it simple} process had 3 binary variables (nodes), which means that the problem can be thought of as the simultaneous estimation of $(2^3) = 8$ parameters (with the help of the correct Bayesian Network, this can be achieved with only 5 parameters). The parameters are in the 8-dimensional simplex, with $0\leq p_i \leq 1$, for all $i=1, \ldots 8$ and such that $\sum_{i=1}^8(p_i) =1$ and thus if the participant counted and kept track of all joint outcomes, only 7 parameters would need to be estimated, as the last one could be deduced as $p_8 =1-\sum_{i=1}^7$.  

For the 4 variable {\bf {\it complex} case, there are 16 possible joint outcomes (15 necessary estimates, using counting of joint outcomes) that can be represented with the correct Baysian Network, using just 7 parameters.\\

Participants were given 40 minutes in total, with a 10 minute warm-up period to get acustomed to the interface (see {\bf Supplementary Information (SI) for screenshots}), and to start building models. The warm-up period was followed by 30 minutes during which participants were rewarded according to their predictions. They were truthfully told that the following sequence--a variation of a wellknown incentive scheme in experimental economics, known as the Becker–DeGroot–Marschak method\citep{becker1964measuring}--would determine their rewards.  First the computer draws a random valuation for the bet that a marked variable, named the betting variable would take on the value ``H''.  The participants model is then used to put a probability on this same event (the marked variable takes on the value ``H'').  If this model-probability is lower than the random value drawn by the computer, the participant automatically receives the value drawn by the computer in cents.  If, however, that probability is higher than the value drawn by the computer, then the participant engages in the following lottery: get one dollar, if the variable comes to take on the value ``H'' and get nothing, if the variable instead takes on the value ``L''. This method has been studied and widely accepted as an incentive for truth-telling in experimental economics, although more recently the validity of this method as a universal truth-telling incentive has been put into question\citet{horowitz2006becker}. \\

What participants can see is the variables, or nodes, in different colors and with labels chosen from macro economics, such as Interest Rate and Industry, but to the best of our ability we stayed clear of politically salient labels, such as Taxes, or Abortion Rate.  They could then click on one such vaiable, effectively declaring it a cause, and then on another, declaring it an effect and an arrow was drawn from the cause to the effect. Double clicking a variable opens a window in which parameters can be set for specific outcomes or conditional outcomes, representing the probabilities. If a variable is an effect variable, with one or multiple causes, conditional probabilities have to be set, such as the Industry variable takes on the value ``H'' with probability 0.6, if the Interest Rate takes on the value ``H'' etc.  At the bottom of the screen a stream of past joint outcomes appears and even from the first second of training there are already 40 previous joint data points visible.  These are of the form (Interest Rate: ``H'', Industry: ``L'', ...) and they show these outcomes with connected lines, colored in the same color as the variables so that correlations can be more easily descerned; these data points were generated by the true process, which participants were truthfully told is a model of the same form as the one that they could build in the provided interface.  The participants could erase an arrow by double clicking on it and this would severe the causal relationship.  Likewise, they could double click on any variable and adjust the parameters that they had previously set.  

All changes made were recorded at a 1 second resolution, which provides unprecedented resolution on the fine-grained human behaviors when facing a ``hard problem".\\
